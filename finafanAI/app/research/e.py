import asyncio
from app.service.search import fast_news_search
from app.service.prompts import idol_news_prompt
from langchain_openai import ChatOpenAI
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
)
from difflib import SequenceMatcher
from datasets import Dataset
import pandas as pd
import matplotlib.pyplot as plt

# âœ… LLM & ì²´ì¸
llm = ChatOpenAI(temperature=0.1)
idol_news_chain = idol_news_prompt | llm
sample_questions = [
    "ì´ì°¬ì› ìš”ì¦˜ ë­í•´?",
    "ì„ì˜ì›… ìš”ì¦˜ ë­í•´?",
    "ì˜ì›…ì´ ì†Œì‹ ì•Œë ¤ì¤˜",
    "ì„ì˜ì›… ë‰´ìŠ¤ ì°¾ì•„ì¤˜",
    "ì •ë™ì› ë‰´ìŠ¤ ì°¾ì•„",
    "ë°•ì§€ë¯¼ ìµœê·¼ ê¸°ì‚¬ ìˆì–´?",
    "ë°©íƒ„ì†Œë…„ë‹¨ ê´€ë ¨ ë‰´ìŠ¤ ë³´ì—¬ì¤˜",
    "ì•„ì´ìœ  ë‰´ìŠ¤ ë­ ë–´ì–´?",
    "ì´ì°¬ì› ê´€ë ¨ ê¸°ì‚¬ ì°¾ì•„ì¤˜",
    "ì¥ì›ì˜ ê·¼í™© ê¸°ì‚¬ ì•Œë ¤ì¤˜",
    "ì§€ë¯¼ì´ ë¬´ìŠ¨ ë‰´ìŠ¤ ìˆì–´?",
    "ì •êµ­ ê¸°ì‚¬ ì¢€ ì¤˜",
    "ìˆ˜ì§€ ìš”ì¦˜ ë­í•˜ê³  ìˆì–´?",
    "ì„ì˜ì›… ê¸°ì‚¬ ë­ ë‚˜ì™”ì–´?",
    "ì´ì°¬ì› ê·¼í™© ì•Œë ¤ì¤˜",
    "ì•„ì´ë¸Œ ë‰´ìŠ¤ ë³´ì—¬ì¤˜",
    "ì—”í•˜ì´í”ˆ ê´€ë ¨ ë‰´ìŠ¤ ìˆì„ê¹Œ?",
    "ê¹€ì„¸ì • ìµœê·¼ ê¸°ì‚¬ ë­ ìˆì–´?",
    "ì—ìŠ¤íŒŒ ë‰´ìŠ¤ ë³´ì—¬ì¤˜",
    "ë¥´ì„¸ë¼í•Œ ê¸°ì‚¬ ì¢€ ì°¾ì•„ì¤˜"
]

def print_answer_comparisons(samples):
    print("\nğŸ“Œ ëª¨ë¸ ë‹µë³€ vs ê¸°ì¤€ ì •ë‹µ ë¹„êµ\n" + "-"*40)
    for i, s in enumerate(samples):
        question = s["question"]
        answer = s["answer"]
        ground_truth = s["ground_truth"]

        similarity = SequenceMatcher(None, answer, ground_truth).ratio()

        print(f"[{i+1}] ì§ˆë¬¸: {question}")
        print(f"ğŸ”¹ ëª¨ë¸ ë‹µë³€:      {answer}")
        print(f"ğŸ”¸ ê¸°ì¤€ ì •ë‹µ(GT): {ground_truth}")
        print(f"âœ… ìœ ì‚¬ë„: {similarity:.2f}")
        print("-" * 60)
# âœ… í‰ê°€ìš© ìƒ˜í”Œ ìƒì„± í•¨ìˆ˜
async def build_ragas_sample(question: str):
    try:
        docs = fast_news_search(question) or ["(ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.)"]
        contexts = docs[:3]

        input_data = {
            "topic": question,
            "results": docs
        }

        response = await idol_news_chain.ainvoke(input_data)
        answer = response.content.strip()

        # âœ… ìë™ ê¸°ì¤€ ì •ë‹µ ìƒì„± (ë‰´ìŠ¤ ì²« ì¤„ ê¸°ì¤€)
        ground_truth = contexts[0].split(".")[0] if contexts else "(ê¸°ì¤€ ì—†ìŒ)"

        return {
            "question": question,
            "contexts": contexts if isinstance(contexts, list) else [str(contexts)],
            "answer": answer,
            "ground_truth": ground_truth
        }

    except Exception as e:
        print(f"âŒ [{question}] ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return None

# âœ… í‰ê°€ ë£¨í”„
async def run_ragas_evaluation():
    print("ğŸš€ ë‰´ìŠ¤ ê¸°ë°˜ ì²´ì¸ í‰ê°€ ì‹œì‘...")

    # ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸ëŠ” ì™¸ë¶€ì—ì„œ ë°›ê±°ë‚˜ ë”°ë¡œ ì •ì˜ëœë‹¤ê³  ê°€ì •

    samples = await asyncio.gather(*[build_ragas_sample(q) for q in sample_questions])
    samples = [s for s in samples if s]

    if not samples:
        print("âš ï¸ ìœ íš¨í•œ ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤. í‰ê°€ ì¢…ë£Œ.")
        return

    # âœ… í‰ê°€ìš© Dataset ìƒì„±
    df = pd.DataFrame(samples)
    ragas_dataset = Dataset.from_pandas(df)

    print(f"\nâœ… ì´ {len(ragas_dataset)}ê°œ ìƒ˜í”Œ í‰ê°€ ì¤€ë¹„ ì™„ë£Œ")

    # âœ… í‰ê°€ ì‹¤í–‰
    results = evaluate(
        ragas_dataset,
        metrics=[
            faithfulness,
            answer_relevancy,
            context_precision,
            context_recall,
        ]
    )

   
    import matplotlib.pyplot as plt

# ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜
    scores_dict = results.scores

# íŒë‹¤ìŠ¤ë¡œ ë³€í™˜
    scores_df = pd.DataFrame(scores_dict)
    mean_scores = scores_df.mean().reset_index()
    mean_scores.columns = ["Metric", "Score"]

# ì‹œê°í™”
    mean_scores.plot(
        kind='bar',
        x="Metric",
        y="Score",
        legend=False,
        figsize=(8, 5),
        title="RAGAS Evaluation Metrics (Average)",
        ylim=(0, 1),
        color=["skyblue"]
    )

    plt.ylabel("Average Score")
    plt.xticks(rotation=0)
    plt.grid(axis="y", linestyle="--")
    plt.tight_layout()
    plt.show()
    print_answer_comparisons(samples)
    # âœ… CSV ì €ì¥
    df.to_csv("idol_news_ragas_dataset.csv", index=False)
    scores_df.to_csv("idol_news_ragas_metrics.csv", index=False)

# âœ… ì‹¤í–‰
if __name__ == "__main__":
    asyncio.run(run_ragas_evaluation())
