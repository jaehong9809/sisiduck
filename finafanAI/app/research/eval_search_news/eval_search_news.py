import asyncio
import pandas as pd
from datasets import Dataset
from difflib import SequenceMatcher
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
)
from app.service.v1.prompts import idol_news_prompt
from langchain_openai import ChatOpenAI
from langchain_core.runnables import Runnable
import matplotlib.pyplot as plt
import re

# âœ… LLM & ì²´ì¸ êµ¬ì„±
llm = ChatOpenAI(temperature=0.1)
idol_news_chain: Runnable = idol_news_prompt | llm

# âœ… ë•ìˆœì´ í”„ë¡¬í”„íŠ¸ ì¶œë ¥ì—ì„œ ìš”ì•½ë§Œ ì¶”ì¶œ
def clean_model_answer(raw_answer: str) -> str:
    """
    ë•ìˆœì´ ìŠ¤íƒ€ì¼ ë‹µë³€ì—ì„œ [LINK] ì´í›„ ì œê±°
    """
    cleaned = re.split(r"\[LINK\]", raw_answer)[0]
    return cleaned.strip()

# âœ… ëª¨ë¸ ì‘ë‹µ + í‰ê°€ìš© ìƒ˜í”Œ êµ¬ì„±
async def build_ragas_sample(row):
    try:
        question = row["question"]
        contexts = row["news_titles"].split("\n")
        ground_truth = row["ground_truth"]

        input_data = {
            "topic": question,
            "results": contexts
        }

        response = await idol_news_chain.ainvoke(input_data)
        raw_answer = response.content.strip()
        answer = clean_model_answer(raw_answer)

        return {
            "question": question,
            "contexts": contexts,
            "answer": answer,
            "raw_answer": raw_answer,
            "ground_truth": ground_truth
        }

    except Exception as e:
        print(f"âŒ [{row['question']}] ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return None

# âœ… ëª¨ë¸ ë‹µë³€ vs ì •ë‹µ ë¹„êµ ì¶œë ¥
def print_answer_comparisons(samples):
    print("\nğŸ“Œ ëª¨ë¸ ìš”ì•½ vs ê¸°ì¤€ ì •ë‹µ ë¹„êµ\n" + "-"*50)
    for i, s in enumerate(samples):
        question = s["question"]
        answer = s["answer"]
        ground_truth = s["ground_truth"]
        similarity = SequenceMatcher(None, answer, ground_truth).ratio()

        print(f"[{i+1}] ì§ˆë¬¸: {question}")
        print(f"ğŸ”¹ ëª¨ë¸ ìš”ì•½:   {answer}")
        print(f"ğŸ”¸ ê¸°ì¤€ ì •ë‹µ:   {ground_truth}")
        print(f"âœ… ìœ ì‚¬ë„: {similarity:.2f}")
        print("-" * 60)

# âœ… í‰ê°€ ë£¨í”„
async def run_ragas_evaluation():
    print("ğŸš€ Ground Truth ê¸°ë°˜ RAG í‰ê°€ ì‹œì‘...\n")

    # âœ… CSV ë¡œë“œ
    raw_df = pd.read_excel("app/research/generated_gt_dataset.xlsx")
    print(f"ğŸ“ ì´ {len(raw_df)}ê°œ ì§ˆë¬¸ ë¡œë“œ ì™„ë£Œ")

    # âœ… ëª¨ë¸ ë‹µë³€ ìƒì„±
    tasks = [build_ragas_sample(row) for _, row in raw_df.iterrows()]
    samples = await asyncio.gather(*tasks)
    samples = [s for s in samples if s]

    if not samples:
        print("âš ï¸ ìœ íš¨í•œ ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤. í‰ê°€ ì¢…ë£Œ")
        return

    # âœ… í‰ê°€ìš© ë°ì´í„°ì…‹ ë³€í™˜
    df = pd.DataFrame(samples)
    ragas_dataset = Dataset.from_pandas(df)

    print(f"âœ… ì´ {len(df)}ê°œ ìƒ˜í”Œ í‰ê°€ ì¤€ë¹„ ì™„ë£Œ\n")

    # âœ… RAGAS í‰ê°€ ì‹¤í–‰
    results = evaluate(
        ragas_dataset,
        metrics=[
            faithfulness,
            answer_relevancy,
            context_precision,
            context_recall,
        ]
    )

    # âœ… ì ìˆ˜ ì •ë¦¬
    scores_df = pd.DataFrame(results.scores)
    mean_scores = scores_df.mean().reset_index()
    mean_scores.columns = ["Metric", "Score"]

    # âœ… ì‹œê°í™”
# âœ… ì‹œê°í™”
    ax = mean_scores.plot(
        kind='bar',
        x="Metric",
        y="Score",
        legend=False,
        figsize=(8, 5),
        title="RAGAS Evaluation Metrics (Average)",
        ylim=(0, 1),
        color=["skyblue"]
    )

    # âœ… ìˆ«ì í‘œì‹œ
    for container in ax.containers:
        ax.bar_label(container, fmt="%.2f", label_type="edge", padding=3, fontsize=10)

    plt.ylabel("Average Score")
    plt.xticks(rotation=0)
    plt.grid(axis="y", linestyle="--")
    plt.tight_layout()
    plt.savefig("ragas_evaluation_graph.png", dpi=300)  # ì €ì¥
    plt.show()

    # âœ… ë‹µë³€ ë¹„êµ ì¶œë ¥
    print_answer_comparisons(samples)

    # âœ… ê²°ê³¼ ì €ì¥
    df.to_csv("ragas_evaluation_result_dataset.csv", index=False)
    scores_df.to_csv("ragas_evaluation_metrics.csv", index=False)
    print("\nğŸ“ í‰ê°€ ê²°ê³¼ ì €ì¥ ì™„ë£Œ!")

# âœ… ì‹¤í–‰
if __name__ == "__main__":
    asyncio.run(run_ragas_evaluation())
