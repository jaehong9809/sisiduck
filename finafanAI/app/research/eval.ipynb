{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ff7c53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:38<00:00,  1.27s/it]\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:30<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ ChainRouter ê²°ê³¼: {'faithfulness': 0.9889, 'answer_relevancy': 0.6942}\n",
      "ðŸ”¹ Agent ê²°ê³¼: {'faithfulness': 0.9833, 'answer_relevancy': 0.6946}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_precision\n",
    "from ragas import evaluate\n",
    "import pandas as pd\n",
    "\n",
    "# ì—‘ì…€ íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df = pd.read_excel(\"ëª¨ë¸ ë¹„êµ.xlsx\").head(15)\n",
    "\n",
    "# ChainRouter í‰ê°€ìš© Dataset\n",
    "chainrouter_dataset = Dataset.from_pandas(df.rename(columns={\n",
    "    \"ì§ˆë¬¸\": \"question\",\n",
    "    \"ChainRouter\": \"answer\"\n",
    "}).assign(contexts=lambda x: [[a] for a in x[\"answer\"]]))\n",
    "\n",
    "# Agent í‰ê°€ìš© Dataset\n",
    "agent_dataset = Dataset.from_pandas(df.rename(columns={\n",
    "    \"ì§ˆë¬¸\": \"question\",\n",
    "    \"Agent\": \"answer\"\n",
    "}).assign(contexts=lambda x: [[a] for a in x[\"answer\"]]))\n",
    "\n",
    "# í‰ê°€ ìˆ˜í–‰\n",
    "metrics = [faithfulness, answer_relevancy]\n",
    "chainrouter_result = evaluate(chainrouter_dataset, metrics=metrics)\n",
    "agent_result = evaluate(agent_dataset, metrics=metrics)\n",
    "\n",
    "print(\"ðŸ”¹ ChainRouter ê²°ê³¼:\", chainrouter_result)\n",
    "print(\"ðŸ”¹ Agent ê²°ê³¼:\", agent_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed80e770",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:23<00:00,  1.68s/it]\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:26<00:00,  1.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ ChainRouter (ragas)\n",
      "{'answer_correctness': 0.3977, 'answer_relevancy': 0.7286}\n",
      "\n",
      "ðŸ”¹ Agent (ragas)\n",
      "{'answer_correctness': 0.5776, 'answer_relevancy': 0.6236}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SSAFY\\Desktop\\project_2\\S12P21A702\\finafanAI\\myenv\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\SSAFY\\.cache\\huggingface\\hub\\models--bert-base-multilingual-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ ChainRouter (BERTScore)\n",
      "Precision: 0.6000\n",
      "Recall:    0.6777\n",
      "F1:        0.6345\n",
      "\n",
      "ðŸ”¹ Agent (BERTScore)\n",
      "Precision: 0.6418\n",
      "Recall:    0.7301\n",
      "F1:        0.6823\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_correctness, answer_relevancy\n",
    "import evaluate as hf_evaluate\n",
    "\n",
    "# ì—‘ì…€ íŒŒì¼ ë¡œë“œ\n",
    "df = pd.read_excel(\"aa.xlsx\")\n",
    "\n",
    "# ChainRouter í‰ê°€ìš© ë°ì´í„°ì…‹ êµ¬ì„±\n",
    "chain_ds = Dataset.from_pandas(df.rename(columns={\n",
    "    \"question\": \"question\",\n",
    "    \"ChainRouter\": \"answer\",\n",
    "    \"Ground_thruth\": \"ground_truth\"\n",
    "}).assign(contexts=lambda x: [[a] for a in x[\"answer\"]]))\n",
    "\n",
    "# Agent í‰ê°€ìš© ë°ì´í„°ì…‹ êµ¬ì„±\n",
    "agent_ds = Dataset.from_pandas(df.rename(columns={\n",
    "    \"question\": \"question\",\n",
    "    \"Agent\": \"answer\",\n",
    "    \"Ground_thruth\": \"ground_truth\"\n",
    "}).assign(contexts=lambda x: [[a] for a in x[\"answer\"]]))\n",
    "\n",
    "# ragas í‰ê°€\n",
    "metrics = [answer_correctness, answer_relevancy]\n",
    "chain_result = evaluate(chain_ds, metrics=metrics)\n",
    "agent_result = evaluate(agent_ds, metrics=metrics)\n",
    "\n",
    "print(\"ðŸ”¹ ChainRouter (ragas)\")\n",
    "print(chain_result)\n",
    "\n",
    "print(\"\\nðŸ”¹ Agent (ragas)\")\n",
    "print(agent_result)\n",
    "\n",
    "# -----------------------\n",
    "# BERTScore í‰ê°€ ì¶”ê°€\n",
    "# -----------------------\n",
    "bertscore = hf_evaluate.load(\"bertscore\")\n",
    "\n",
    "# í‰ê°€ìš© í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "chain_preds = df[\"ChainRouter\"].tolist()\n",
    "agent_preds = df[\"Agent\"].tolist()\n",
    "references = df[\"Ground_thruth\"].tolist()\n",
    "\n",
    "# BERTScore í‰ê°€\n",
    "chain_bertscore = bertscore.compute(predictions=chain_preds, references=references, lang=\"ko\")\n",
    "agent_bertscore = bertscore.compute(predictions=agent_preds, references=references, lang=\"ko\")\n",
    "\n",
    "# í‰ê·  ì ìˆ˜ ì¶œë ¥\n",
    "print(\"\\nðŸ”¹ ChainRouter (BERTScore)\")\n",
    "print(f\"Precision: {sum(chain_bertscore['precision'])/len(chain_bertscore['precision']):.4f}\")\n",
    "print(f\"Recall:    {sum(chain_bertscore['recall'])/len(chain_bertscore['recall']):.4f}\")\n",
    "print(f\"F1:        {sum(chain_bertscore['f1'])/len(chain_bertscore['f1']):.4f}\")\n",
    "\n",
    "print(\"\\nðŸ”¹ Agent (BERTScore)\")\n",
    "print(f\"Precision: {sum(agent_bertscore['precision'])/len(agent_bertscore['precision']):.4f}\")\n",
    "print(f\"Recall:    {sum(agent_bertscore['recall'])/len(agent_bertscore['recall']):.4f}\")\n",
    "print(f\"F1:        {sum(agent_bertscore['f1'])/len(agent_bertscore['f1']):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "226bf212",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:30<00:00,  1.00s/it]\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:36<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ ChainRouter (ragas í‰ê°€)\n",
      "{'answer_correctness': 0.3336, 'answer_relevancy': 0.7452}\n",
      "\n",
      "ðŸ”¹ Agent (ragas í‰ê°€)\n",
      "{'answer_correctness': 0.3522, 'answer_relevancy': 0.6954}\n",
      "\n",
      "ðŸ”¹ ChainRouter (BERTScore)\n",
      "Precision: 0.6188\n",
      "Recall:    0.6722\n",
      "F1:        0.6437\n",
      "\n",
      "ðŸ”¹ Agent (BERTScore)\n",
      "Precision: 0.6167\n",
      "Recall:    0.6759\n",
      "F1:        0.6442\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_correctness, answer_relevancy\n",
    "import evaluate as hf_evaluate\n",
    "\n",
    "# ì—‘ì…€ íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df = pd.read_excel(\"ëª¨ë¸ ë¹„êµ.xlsx\").dropna(subset=[\"ChainRouter\", \"Agent\", \"Ground_Truth\"])\n",
    "\n",
    "# ragas í‰ê°€ìš© ë°ì´í„°ì…‹ êµ¬ì„±\n",
    "def make_dataset(answer_column):\n",
    "    return Dataset.from_pandas(df.rename(columns={\n",
    "        \"ì§ˆë¬¸\": \"question\",\n",
    "        answer_column: \"answer\",\n",
    "        \"Ground_Truth\": \"ground_truth\"\n",
    "    }).assign(contexts=lambda x: [[a] for a in x[\"answer\"]]))\n",
    "\n",
    "# ChainRouterì™€ Agentìš© ë°ì´í„°ì…‹\n",
    "chain_ds = make_dataset(\"ChainRouter\")\n",
    "agent_ds = make_dataset(\"Agent\")\n",
    "\n",
    "# ragas í‰ê°€ ì‹¤í–‰\n",
    "metrics = [answer_correctness, answer_relevancy]\n",
    "chain_result = evaluate(chain_ds, metrics=metrics)\n",
    "agent_result = evaluate(agent_ds, metrics=metrics)\n",
    "\n",
    "print(\"ðŸ”¹ ChainRouter (ragas í‰ê°€)\")\n",
    "print(chain_result)\n",
    "\n",
    "print(\"\\nðŸ”¹ Agent (ragas í‰ê°€)\")\n",
    "print(agent_result)\n",
    "\n",
    "# -----------------------------\n",
    "# BERTScore í‰ê°€\n",
    "# -----------------------------\n",
    "bertscore = hf_evaluate.load(\"bertscore\")\n",
    "\n",
    "chain_preds = df[\"ChainRouter\"].tolist()\n",
    "agent_preds = df[\"Agent\"].tolist()\n",
    "references = df[\"Ground_Truth\"].tolist()\n",
    "\n",
    "chain_bertscore = bertscore.compute(predictions=chain_preds, references=references, lang=\"ko\")\n",
    "agent_bertscore = bertscore.compute(predictions=agent_preds, references=references, lang=\"ko\")\n",
    "\n",
    "print(\"\\nðŸ”¹ ChainRouter (BERTScore)\")\n",
    "print(f\"Precision: {sum(chain_bertscore['precision'])/len(chain_bertscore['precision']):.4f}\")\n",
    "print(f\"Recall:    {sum(chain_bertscore['recall'])/len(chain_bertscore['recall']):.4f}\")\n",
    "print(f\"F1:        {sum(chain_bertscore['f1'])/len(chain_bertscore['f1']):.4f}\")\n",
    "\n",
    "print(\"\\nðŸ”¹ Agent (BERTScore)\")\n",
    "print(f\"Precision: {sum(agent_bertscore['precision'])/len(agent_bertscore['precision']):.4f}\")\n",
    "print(f\"Recall:    {sum(agent_bertscore['recall'])/len(agent_bertscore['recall']):.4f}\")\n",
    "print(f\"F1:        {sum(agent_bertscore['f1'])/len(agent_bertscore['f1']):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405c20d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
