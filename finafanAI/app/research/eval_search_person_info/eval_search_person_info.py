import asyncio
import pandas as pd
from datasets import Dataset
from difflib import SequenceMatcher
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
)
from langchain_openai import ChatOpenAI
from langchain_core.runnables import Runnable, RunnableMap
import matplotlib.pyplot as plt

from app.service.v1.search import person_retriever  # ğŸ” ë²¡í„°ì„œì¹˜
from app.service.v1.prompts import person_prompt   # ğŸ” ë•ìˆœì´ ìŠ¤íƒ€ì¼

# âœ… LLM
llm = ChatOpenAI(temperature=0.1)

# âœ… ì²´ì¸ êµ¬ì„± (ì§ˆë¬¸ -> ë¬¸ì„œ -> í”„ë¡¬í”„íŠ¸ ìš”ì•½)
person_chain: Runnable = (
    RunnableMap({
        "context": lambda x: "\n\n".join(
            doc.page_content for doc in person_retriever.get_relevant_documents(x["query"])
        ),
        "person": lambda x: x["query"],
        "question": lambda x: x["query"]
    }) | person_prompt | llm
)

# âœ… ë•ìˆœì´ ìŠ¤íƒ€ì¼ ì¶œë ¥ì—ì„œ ìš”ì•½ë§Œ ì¶”ì¶œ
def clean_model_answer(raw_answer: str) -> str:
    return raw_answer.strip()  # í•„ìš”ì‹œ ì •ì œ ê·œì¹™ ì¶”ê°€

# âœ… ëª¨ë¸ ì‘ë‹µ + í‰ê°€ ìƒ˜í”Œ êµ¬ì„±
async def build_ragas_sample(row):
    try:
        question = row["question"]
        ground_truth = row["ground_truth"]

        response = await person_chain.ainvoke({"query": question})
        raw_answer = response.content.strip()
        answer = clean_model_answer(raw_answer)

        retrieved_docs = person_retriever.get_relevant_documents(question)
        contexts = [doc.page_content for doc in retrieved_docs[:3]]

        return {
            "question": question,
            "contexts": contexts,
            "answer": answer,
            "raw_answer": raw_answer,
            "ground_truth": ground_truth
        }

    except Exception as e:
        print(f"âŒ [{row['question']}] ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return None

# âœ… ëª¨ë¸ vs ê¸°ì¤€ì •ë‹µ ì¶œë ¥
def print_answer_comparisons(samples):
    print("\nğŸ“Œ ëª¨ë¸ ìš”ì•½ vs ê¸°ì¤€ ì •ë‹µ ë¹„êµ\n" + "-"*50)
    for i, s in enumerate(samples):
        question = s["question"]
        answer = s["answer"]
        ground_truth = s["ground_truth"]
        similarity = SequenceMatcher(None, answer, ground_truth).ratio()

        print(f"[{i+1}] ì§ˆë¬¸: {question}")
        print(f"ğŸ”¹ ëª¨ë¸ ìš”ì•½:   {answer}")
        print(f"ğŸ”¸ ê¸°ì¤€ ì •ë‹µ:   {ground_truth}")
        print(f"âœ… ìœ ì‚¬ë„: {similarity:.2f}")
        print("-" * 60)

# âœ… í‰ê°€ ë£¨í”„
async def run_ragas_evaluation():
    print("ğŸš€ ì¸ë¬¼ ê¸°ë°˜ RAG í‰ê°€ ì‹œì‘...\n")

    # âœ… ë°ì´í„° ë¡œë”©
    raw_df = pd.read_excel("app/research/person_gt_dataset.xlsx")
    print(f"ğŸ“ ì´ {len(raw_df)}ê°œ ì§ˆë¬¸ ë¡œë“œ ì™„ë£Œ")

    # âœ… ëª¨ë¸ ë‹µë³€ ìƒì„±
    tasks = [build_ragas_sample(row) for _, row in raw_df.iterrows()]
    samples = await asyncio.gather(*tasks)
    samples = [s for s in samples if s]

    if not samples:
        print("âš ï¸ ìœ íš¨í•œ ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤. í‰ê°€ ì¢…ë£Œ")
        return

    # âœ… í‰ê°€ìš© Dataset ë³€í™˜
    df = pd.DataFrame(samples)
    ragas_dataset = Dataset.from_pandas(df)

    print(f"âœ… ì´ {len(df)}ê°œ ìƒ˜í”Œ í‰ê°€ ì¤€ë¹„ ì™„ë£Œ\n")

    # âœ… í‰ê°€ ì‹¤í–‰
    results = evaluate(
        ragas_dataset,
        metrics=[
            faithfulness,
            answer_relevancy,
            context_precision,
            context_recall,
        ]
    )

    # âœ… ì ìˆ˜ ì‹œê°í™”
    scores_df = pd.DataFrame(results.scores)
    mean_scores = scores_df.mean().reset_index()
    mean_scores.columns = ["Metric", "Score"]

    ax = mean_scores.plot(
        kind='bar',
        x="Metric",
        y="Score",
        legend=False,
        figsize=(8, 5),
        title="RAGAS person information RAG evaluation results(avg)",
        ylim=(0, 1),
        color=["skyblue"]
    )

    for container in ax.containers:
        ax.bar_label(container, fmt="%.2f", label_type="edge", padding=3, fontsize=10)

    plt.ylabel("Average Score")
    plt.xticks(rotation=0)
    plt.grid(axis="y", linestyle="--")
    plt.tight_layout()
    plt.savefig("person_ragas_evaluation_graph.png", dpi=300)
    plt.show()

    print_answer_comparisons(samples)

    # âœ… ì €ì¥
    df.to_csv("person_ragas_result_dataset.csv", index=False)
    scores_df.to_csv("person_ragas_metrics.csv", index=False)
    print("\nğŸ“ í‰ê°€ ê²°ê³¼ ì €ì¥ ì™„ë£Œ!")

# âœ… ì‹¤í–‰
if __name__ == "__main__":
    asyncio.run(run_ragas_evaluation())
