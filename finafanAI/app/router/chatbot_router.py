from fastapi import APIRouter
from fastapi.responses import StreamingResponse
from app.schemas.question import QuestionRequest
from app.service.service import classify_query
from app.service.prompts import idol_news_prompt, idol_video_prompt, finance_prompt, default_prompt, person_prompt
from app.service.search import fast_news_search, youtube_search, person_retriever
from app.service.callback_handler import SSECallbackHandler
from app.core.redis_utils import make_cache_key, get_cached_response, set_cached_response
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnableBranch, RunnableMap

import asyncio

load_dotenv()
router = APIRouter()

@router.post("")
async def ask_question(request: QuestionRequest):
    question = request.question
    cache_key = make_cache_key(question)

    # âœ… 1. Redis ìºì‹œ í™•ì¸
    cached = await get_cached_response(cache_key)
    if cached:
        async def stream_cached():
            for word in cached.split():
                yield f"data: {word} \n\n"
                await asyncio.sleep(0.015)  # ì•½ê°„ì˜ ë”œë ˆì´ë¡œ ìì—°ìŠ¤ëŸ¬ì›€
        return StreamingResponse(stream_cached(), media_type="text/event-stream")

    # âœ… 2. ìºì‹œ miss â†’ ìƒˆë¡œ ì²˜ë¦¬
    callback = SSECallbackHandler()
    collected = []  # ì „ì²´ ì‘ë‹µ ì €ì¥ìš©

    # âœ… ì½œë°±ì— í† í° ìˆ˜ì§‘ ê¸°ëŠ¥ ì¶”ê°€
    original_on_token = callback.on_llm_new_token
    async def capture_and_stream(token: str, **kwargs):
        collected.append(token)
        await original_on_token(token, **kwargs)
    callback.on_llm_new_token = capture_and_stream

    # âœ… LLM & ì²´ì¸ ì„¸íŒ…
    llm = ChatOpenAI(
        model="gpt-3.5-turbo",
        temperature=0.5,
        streaming=True,
        callbacks=[callback]
    )

    idol_news_chain = idol_news_prompt | llm
    idol_video_chain = idol_video_prompt | llm
    finance_chain = finance_prompt | llm
    default_chain = default_prompt | llm

    person_chain = (
        RunnableMap({
            "context": lambda x: log_and_return_context(x),
            "person": lambda x: x["query"],
            "question": lambda x: x["query"]
        }) | person_prompt | llm
    )

    def log_and_return_context(x):
        docs = person_retriever.get_relevant_documents(x["query"])
        print("ğŸ” ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸:")
        for i, doc in enumerate(docs, start=1):
            print(f"\nğŸ“„ ë¬¸ì„œ {i}:\n{doc.page_content}\n{'-'*40}")
        return "\n\n".join(doc.page_content for doc in docs)

    # âœ… ë¶„ê¸° ì²˜ë¦¬ìš© í•¨ìˆ˜ë“¤ ì •ì˜
    async def handle_video(x):
        return await idol_video_chain.ainvoke({
            "topic": x["query"],
            "results": youtube_search(x["query"])
        })

    async def handle_news(x):
        return await idol_news_chain.ainvoke({
            "topic": x["query"],
            "results": fast_news_search(x["query"])
        })

    async def handle_finance(x):
        return await finance_chain.ainvoke({
            "topic": x["query"],
            "results": fast_news_search(x["query"])
        })

    async def handle_default(x):
        return await default_chain.ainvoke({
            "input": x.get("input", "")
        })
    
    async def handle_person(x):
        return await person_chain.ainvoke(x)

    # âœ… RunnableBranch ì„¤ì •
    router_chain = RunnableBranch(
        (lambda x: x.get("type") == "video", handle_video),
        (lambda x: x.get("type") == "news", handle_news),
        (lambda x: x.get("type") == "finance", handle_finance),
        (lambda x: x.get("type") == "person", handle_person),  # âœ… ì¶”ê°€!
        handle_default
    )

    # âœ… ì²´ì¸ ì‹¤í–‰
    async def run_chain():
        try:
            input_data = classify_query(question)
            await router_chain.ainvoke(input_data)
        except Exception as e:
            print(f"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
            await callback.queue.put("ì£„ì†¡í•©ë‹ˆë‹¤, ì²˜ë¦¬ê°€ ì–´ë ¤ì›Œìš”.\n")
        finally:
            callback.finish()

    # âœ… ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ + ìºì‹œ ì €ì¥
    async def event_stream():
        await run_chain()

        # Redisì— ì „ì²´ ì‘ë‹µ ìºì‹±
        full_response = "".join(collected)
        await set_cached_response(cache_key, full_response)

        async for token in callback.token_generator():
            yield token

    return StreamingResponse(event_stream(), media_type="text/event-stream")
