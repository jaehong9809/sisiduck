from langchain.memory import ConversationBufferMemory
from langchain.agents import Tool, AgentExecutor, create_react_agent
from langchain_core.runnables import RunnableLambda
from langchain_core.messages import HumanMessage, SystemMessage
import re
from app.service.v2.search import fast_news_search, youtube_search, duckduckgo_search, get_weather, read_webpage
from app.service.v2.prompts import DUKSUNI_SYSTEM_PROMPT
from app.service.v2.llm import get_llm, get_hard_llm, get_soft_llm
from langchain import hub

# âœ… ë©”ëª¨ë¦¬
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True, output_key="output")

# âœ… ë„êµ¬ ì •ì˜
tools = [
    Tool(name="ë‰´ìŠ¤ê²€ìƒ‰", func=fast_news_search, description="ìµœì‹  ë‰´ìŠ¤, ì—°ì˜ˆì¸ ê¸°ì‚¬ ë“± ì™¸ë¶€ ì •ë³´ë¥¼ ì°¾ì„ ë•Œ ì‚¬ìš©"),
    Tool(name="ë™ì˜ìƒê²€ìƒ‰", func=youtube_search, description="YouTubeì—ì„œ ì˜ìƒì´ë‚˜ ë°©ì†¡ í´ë¦½ì„ ë³´ê³  ì‹¶ì„ ë•Œ ì‚¬ìš©"),
    Tool(name="ì›¹ê²€ìƒ‰", func=duckduckgo_search, description="ì¼ë°˜ ì—°ì˜ˆì¸ ì •ë³´, ë¸”ë¡œê·¸, ì»¤ë®¤ë‹ˆí‹° ë“± ì „ì²´ ì›¹ ê²€ìƒ‰ì´ í•„ìš”í•  ë•Œ ì‚¬ìš©"),
    Tool(name="ë‚ ì”¨ê²€ìƒ‰", func=get_weather, description="í˜„ì¬ ë‚ ì”¨, ê¸°ì˜¨, ìŠµë„ ë“± ë‚ ì”¨ ì •ë³´ê°€ í•„ìš”í•  ë•Œ ì‚¬ìš©"),
    Tool(name="ì›¹í˜ì´ì§€ì½ê¸°", func=read_webpage, description="ë§í¬ëœ ì›¹í˜ì´ì§€ì˜ ì‹¤ì œ ë‚´ìš©ì„ ì½ê³  ì‹¶ì„ ë•Œ ì‚¬ìš©")
]

# âœ… ë§íˆ¬ ë³€í™˜
async def to_friendly_tone(answer: str) -> str:
    llm = get_soft_llm(streaming=False)
    prompt = [
        SystemMessage(content=DUKSUNI_SYSTEM_PROMPT.strip()),
        HumanMessage(content=f'ë‹¤ìŒ ë‚´ìš©ì„ ì¹œêµ¬ì²˜ëŸ¼ ë°˜ë§ë¡œ ë°”ê¿”ì¤˜. \n\n"{answer}"\n\në³€í™˜ëœ ë§íˆ¬:')
    ]
    result = await llm.ainvoke(prompt)
    return result.content.strip()

# âœ… Final Answer ì¶”ì¶œ
def extract_final_answer(text: str) -> str:
    if "Final Answer:" in text:
        return text.split("Final Answer:")[-1].strip()
    return text.strip()

# âœ… Agent íŒë‹¨ í•¨ìˆ˜
def needs_agent(input: dict) -> bool:
    question = input["input"].strip()

    keywords = ["ë‚ ì”¨", "ì˜¨ë„", "ìŠµë„", "ì¶”ì›Œ", "ë”ì›Œ"]
    if any(keyword in question for keyword in keywords):
        return True

    check_prompt = f"""
        ë‹¤ìŒ ì§ˆë¬¸ì€ ê²€ìƒ‰ ë„êµ¬(ë‰´ìŠ¤, ìœ íŠœë¸Œ, ì¸ë¬¼ì •ë³´, ë‚ ì”¨ ë“±)ë¥¼ ì‚¬ìš©í•´ì•¼ í•˜ë‚˜ìš”?

        â€» ì§ˆë¬¸ì— ì—°ì˜ˆì¸, ìœ ëª…ì¸, ìœ íŠœë²„, ë°©ì†¡ì¸ ë“± ì¸ë¬¼ì— ëŒ€í•œ ì •ë³´ê°€ ë“¤ì–´ìˆìœ¼ë©´ YESë¡œ íŒë‹¨í•´ì£¼ì„¸ìš”.

        ê²€ìƒ‰ ë„êµ¬ê°€ í•„ìš”í•œ ì§ˆë¬¸ì´ë©´ YES,  
        ê·¸ëƒ¥ ì¼ìƒì ì¸ ëŒ€í™”ë©´ NOë§Œ ì¶œë ¥í•´ì£¼ì„¸ìš”.

        ì§ˆë¬¸: "{question}"
        ë‹µë³€:
        """
    
    result = get_hard_llm(streaming=False).invoke(check_prompt)
    response = result.content.strip().upper()
    
    print(f"[ğŸ” needs_agent íŒë‹¨] â†’ {response}")

    return response == "YES"

# âœ… Chat ì²´ì¸ ìƒì„± í•¨ìˆ˜ (í›„ì²˜ë¦¬ ë²„ì „)
def get_chat_chain(callback):
    async def _chat(x):
        llm = get_soft_llm(streaming=True, callback=callback)

        result = await llm.ainvoke([
            SystemMessage(content=DUKSUNI_SYSTEM_PROMPT.strip()),
            HumanMessage(content=x["input"])
        ])
        print("\nğŸ§© [Chat ì›ë³¸ ì¶œë ¥]")
        print(result.content)

        return {"output": result.content}

    return RunnableLambda(_chat)

# âœ… Agent ì²´ì¸ ìƒì„± í•¨ìˆ˜ (í›„ì²˜ë¦¬ ë²„ì „)
def get_agent_chain(callback):
    async def _agent(x):
        llm = get_llm(streaming=True, callback=callback)

        # prompt = react_prompt_kr.partial(
        #     tools="\n".join([f"{t.name}: {t.description}" for t in tools]),
        #     tool_names=", ".join([t.name for t in tools])
        # )
        # prompt = ReActPrompt().get_prompt(tools)

        prompt2 = hub.pull("hwchase17/react")

        agent = create_react_agent(llm=llm, tools=tools, prompt=prompt2)

        executor = AgentExecutor(
            agent=agent,
            tools=tools,
            memory=memory,
            handle_parsing_errors=True,
            verbose=True,
            max_iterations=8,
            return_intermediate_steps=True,
            output_key="output"
        )

        result = await executor.ainvoke(x)
        
        intermediate_steps = result.get("intermediate_steps", [])
        seen = set()
        unique_tool_outputs = []

        # for step in intermediate_steps:
        #     action = step[0]  # AgentAction
        #     output = step[1]  # Tool output (observation)
        #     # Exceptionì€ ë¬´ì‹œ
        #     if action.tool == "_Exception":
        #         continue

        #     key = (action.tool, str(action.tool_input))
        #     if key not in seen:
        #         seen.add(key)
        #         unique_tool_outputs.append(output)

        # search_summary = "\n".join(unique_tool_outputs)

        final_answer = extract_final_answer(result["output"])
        if final_answer =="Agent stopped due to iteration limit or time limit.":
            final_answer = "ë¯¸ì•ˆ ë‹¤ì‹œ ë¬¼ì–´ë´ì¤˜"
            
        # ì „ì²´ ë§¥ë½ ì—°ê²°
        #full_context = final_answer + "\n\n" + search_summary
        
        print("\nğŸª„ [Final Answer ì¶”ì¶œ ê²°ê³¼]")
        print(final_answer)

        friendly = await to_friendly_tone(final_answer)
        print("\nğŸ’¬ [ë•ìˆœì´ ë§íˆ¬ ë³€í™˜ ê²°ê³¼]")
        print(friendly)

        return {"output": friendly}

    return RunnableLambda(_agent)
