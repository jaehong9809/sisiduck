from langchain.memory import ConversationBufferMemory
from langchain.agents import Tool, AgentExecutor, create_react_agent
from langchain_core.runnables import RunnableLambda
from langchain_core.messages import HumanMessage, SystemMessage

from app.service.v2.search2 import fast_news_search, youtube_search, search_person_info, duckduckgo_search, get_weather
from app.service.v2.prompts2 import DUKSUNI_SYSTEM_PROMPT, react_prompt_kr
from app.service.v2.llm import get_llm, get_hard_llm
# âœ… ë©”ëª¨ë¦¬
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True, output_key="output")

# âœ… ë„êµ¬ ì •ì˜
tools = [
    Tool(name="ë‰´ìŠ¤ê²€ìƒ‰", func=fast_news_search, description="ë‰´ìŠ¤, ê¸°ì‚¬, ì´ìŠˆ ë“± ì™¸ë¶€ ì •ë³´ë¥¼ ì°¾ì„ ë•Œ ì‚¬ìš©"),
    Tool(name="ë™ì˜ìƒê²€ìƒ‰", func=youtube_search, description="YouTubeì—ì„œ ì˜ìƒì´ë‚˜ ë°©ì†¡ í´ë¦½ì„ ë³´ê³  ì‹¶ì„ ë•Œ ì‚¬ìš©"),
    Tool(name="ì¸ë¬¼ì •ë³´ê²€ìƒ‰", func=search_person_info, description="ìƒì¼, MBTI, ê³ í–¥ ë“± ê¸°ë³¸ ì •ë³´ê°€ í•„ìš”í•  ë•Œ ì‚¬ìš©"),
    Tool(name="ì›¹ê²€ìƒ‰", func=duckduckgo_search, description="ì¼ë°˜ ì›¹ ì—°ì˜ˆì¸ ì •ë³´, ë¸”ë¡œê·¸, ì»¤ë®¤ë‹ˆí‹° ë“± ì „ì²´ ì›¹ ê²€ìƒ‰ì´ í•„ìš”í•  ë•Œ ì‚¬ìš©"),
    Tool(name="ë‚ ì”¨ê²€ìƒ‰", func=get_weather, description="í˜„ì¬ ë‚ ì”¨, ê¸°ì˜¨, ìŠµë„ ë“± ë‚ ì”¨ ì •ë³´ê°€ í•„ìš”í•  ë•Œ ì‚¬ìš©"
)
]

# âœ… ë§íˆ¬ ë³€í™˜
async def to_friendly_tone(answer: str) -> str:
    llm = get_hard_llm(streaming=False)
    prompt = [
        SystemMessage(content=DUKSUNI_SYSTEM_PROMPT.strip()),
        HumanMessage(content=f'ë‹¤ìŒ ë‚´ìš©ì„ ì¹œêµ¬ì²˜ëŸ¼ ë°˜ë§ë¡œ ë°”ê¿”ì¤˜. ì‚´ì§ ì£¼ì ‘ ì„ì–´ë„ ì¢‹ì•„~\n\n"{answer}"\n\në³€í™˜ëœ ë§íˆ¬:')
    ]
    result = await llm.ainvoke(prompt)
    return result.content.strip()

# âœ… Final Answer ì¶”ì¶œ
def extract_final_answer(text: str) -> str:
    if "Final Answer:" in text:
        return text.split("Final Answer:")[-1].strip()
    return text.strip()

# âœ… Agent íŒë‹¨ í•¨ìˆ˜
def needs_agent(input: dict) -> bool:
    question = input["input"].strip()
    check_prompt = f"""
        ë‹¤ìŒ ì§ˆë¬¸ì€ ê²€ìƒ‰ ë„êµ¬(ë‰´ìŠ¤, ìœ íŠœë¸Œ, ì¸ë¬¼ì •ë³´, ë‚ ì”¨ ë“±)ë¥¼ ì‚¬ìš©í•´ì•¼ í•˜ë‚˜ìš”?

        â€» ì§ˆë¬¸ì— ì—°ì˜ˆì¸, ìœ ëª…ì¸, ìœ íŠœë²„, ë°©ì†¡ì¸ ë“± ì¸ë¬¼ì— ëŒ€í•œ ì •ë³´ê°€ ë“¤ì–´ìˆìœ¼ë©´ YESë¡œ íŒë‹¨í•´ì£¼ì„¸ìš”.

        ê²€ìƒ‰ ë„êµ¬ê°€ í•„ìš”í•œ ì§ˆë¬¸ì´ë©´ YES,  
        ê·¸ëƒ¥ ì¼ìƒì ì¸ ëŒ€í™”ë©´ NOë§Œ ì¶œë ¥í•´ì£¼ì„¸ìš”.

        ì§ˆë¬¸: "{question}"
        ë‹µë³€:
        """
    
    result = get_llm(streaming=False).invoke(check_prompt)
    response = result.content.strip().upper()
    
    print(f"[ğŸ” needs_agent íŒë‹¨] â†’ {response}")

    return response == "YES"

# âœ… Chat ì²´ì¸ ìƒì„± í•¨ìˆ˜ (í›„ì²˜ë¦¬ ë²„ì „)
def get_chat_chain(callback):
    async def _chat(x):
        llm = get_llm(streaming=True, callback=callback)

        result = await llm.ainvoke([
            SystemMessage(content=DUKSUNI_SYSTEM_PROMPT.strip()),
            HumanMessage(content=x["input"])
        ])
        print("\nğŸ§© [Chat ì›ë³¸ ì¶œë ¥]")
        print(result.content)

        friendly = await to_friendly_tone(result.content)
        print("\nğŸ’¬ [ë•ìˆœì´ ë§íˆ¬ ë³€í™˜ ê²°ê³¼]")
        print(friendly)

        return {"output": friendly}

    return RunnableLambda(_chat)

# âœ… Agent ì²´ì¸ ìƒì„± í•¨ìˆ˜ (í›„ì²˜ë¦¬ ë²„ì „)
def get_agent_chain(callback):
    async def _agent(x):
        llm = get_llm(streaming=True, callback=callback)

        prompt = react_prompt_kr.partial(
            tools="\n".join([f"{t.name}: {t.description}" for t in tools]),
            tool_names=", ".join([t.name for t in tools])
        )

        agent = create_react_agent(llm=llm, tools=tools, prompt=prompt)
        executor = AgentExecutor(
            agent=agent,
            tools=tools,
            memory=memory,
            verbose=True,
            handle_parsing_errors=True,
            max_iterations=4,
            return_intermediate_steps=True,
            output_key="output"
        )

        result = await executor.ainvoke(x)
        intermediate_steps = result.get("intermediate_steps", [])

        # ì „ì²´ Tool ì¶œë ¥ë§Œ ë¦¬ìŠ¤íŠ¸ë¡œ ì¶”ì¶œ
        tool_outputs = [step[1] for step in intermediate_steps]

        # ë¬¸ìì—´ í•˜ë‚˜ë¡œ ë³‘í•©
        search_summary = "\n".join(tool_outputs)
        final_answer = extract_final_answer(result["output"])

        # ì „ì²´ ë§¥ë½ ì—°ê²°
        full_context = search_summary + "\n\n" + final_answer

        print("\nğŸ§© [Agent ì›ë³¸ ì¶œë ¥]")
        print(result["output"])

        final_answer = extract_final_answer(result["output"])
        print("\nğŸª„ [Final Answer ì¶”ì¶œ ê²°ê³¼]")
        print(full_context)
        friendly = await to_friendly_tone(full_context)
        
        print("\nğŸ’¬ [ë•ìˆœì´ ë§íˆ¬ ë³€í™˜ ê²°ê³¼]")
        print(friendly)

        return {"output": friendly}

    return RunnableLambda(_agent)
