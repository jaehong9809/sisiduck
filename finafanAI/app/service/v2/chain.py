from langchain.memory import ConversationBufferMemory
from langchain.agents import Tool, AgentExecutor, create_react_agent
from langchain_core.runnables import RunnableLambda
from langchain_core.messages import HumanMessage, SystemMessage
import re
from app.service.v2.search import (
    fast_news_search,
    youtube_search,
    duckduckgo_search,
    get_weather,
    read_webpage,
    search_person_info
)
from app.service.v2.prompts import DUKSUNI_SYSTEM_PROMPT
from app.service.v2.llm import get_llm, get_hard_llm, get_soft_llm
from langchain import hub
from app.core.conv_utils import get_user_memory


# âœ… ë©”ëª¨ë¦¬
# memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True, output_key="output")

# âœ… ë„êµ¬ ì •ì˜
tools = [
    Tool(
        name="ë‰´ìŠ¤ê²€ìƒ‰",
        func=fast_news_search,
        description="ìµœì‹  ë‰´ìŠ¤, ì—°ì˜ˆì¸ ê¸°ì‚¬, ì†Œì‹ ë“± ì™¸ë¶€ ì •ë³´ë¥¼ ì°¾ì„ ë•Œ ì‚¬ìš©",
    ),
    Tool(
        name="ë™ì˜ìƒê²€ìƒ‰",
        func=youtube_search,
        description="YouTubeì—ì„œ ì˜ìƒì´ë‚˜ ë°©ì†¡ í´ë¦½ì„ ë³´ê³  ì‹¶ì„ ë•Œ ì‚¬ìš©",
    ),
    Tool(
        name="ì—°ì˜ˆì¸ì •ë³´ê²€ìƒ‰",
        func=search_person_info,
        description="ì—°ì˜ˆì¸ì˜ ê¸°ë³¸ ì •ë³´ë‚˜ ê²½ë ¥, ë‚˜ì´, ë°ë·” ì •ë³´ ë“±ì´ í•„ìš”í•  ë•Œ ì‚¬ìš©. ì •ë³´ë¥¼ ëª» ì°¾ì„ ê²½ìš° 'ì›¹ê²€ìƒ‰' ì‚¬ìš©"
    ),
    Tool(
        name="ì›¹ê²€ìƒ‰",
        func=duckduckgo_search,
        description="ë‹¤ë¥¸ íˆ´ë¡œ ì›í•˜ëŠ” ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆì„ ë•Œ ì „ì²´ ì›¹ì—ì„œ í­ë„“ê²Œ ê²€ìƒ‰. ì˜ˆ: ì»¤ë®¤ë‹ˆí‹° ë°˜ì‘, ë¸”ë¡œê·¸ ê¸€, ë˜ëŠ” ì—°ì˜ˆì¸ ì •ë³´ê°€ ì—†ì„ ë•Œ ë³´ì¡°ì ìœ¼ë¡œ ì‚¬ìš©",
    ),
    Tool(
        name="ë‚ ì”¨ê²€ìƒ‰",
        func=get_weather,
        description="í˜„ì¬ ë‚ ì”¨, ê¸°ì˜¨, ìŠµë„ ë“± ë‚ ì”¨ ì •ë³´ê°€ í•„ìš”í•  ë•Œ ì‚¬ìš©",
    ),
    Tool(
        name="ì›¹í˜ì´ì§€ì½ê¸°",
        func=read_webpage,
        description="ë§í¬ëœ ì›¹í˜ì´ì§€ì˜ ì‹¤ì œ ë‚´ìš©ì„ ì½ê³  ì‹¶ì„ ë•Œ ì‚¬ìš©",
    )
]


# âœ… ë§íˆ¬ ë³€í™˜
async def to_friendly_tone(answer: str) -> str:
    llm = get_soft_llm(streaming=False)
    prompt = [
        SystemMessage(content=DUKSUNI_SYSTEM_PROMPT.strip()),
        HumanMessage(
            content=f'ë‹¤ìŒ ë‚´ìš©ì„ ì¹œêµ¬ì²˜ëŸ¼ ë°˜ë§ë¡œ ë°”ê¿”ì¤˜. \n\n"{answer}"\n\në³€í™˜ëœ ë§íˆ¬:'
        ),
    ]
    result = await llm.ainvoke(prompt)
    return result.content.strip()


def get_user_id(x: dict) -> str:
    return x.get("user_id", "test")


# âœ… Final Answer ì¶”ì¶œ
def extract_final_answer(text: str) -> str:
    if "Final Answer:" in text:
        return text.split("Final Answer:")[-1].strip()
    return text.strip()


# âœ… Agent íŒë‹¨ í•¨ìˆ˜
def needs_agent(input: dict) -> bool:
    question = input["input"].strip()

    keywords = ["ë‚ ì”¨", "ì˜¨ë„", "ìŠµë„", "ì¶”ì›Œ", "ë”ì›Œ"]
    if any(keyword in question for keyword in keywords):
        return True

    check_prompt = f"""
        ë‹¤ìŒ ì§ˆë¬¸ì€ ê²€ìƒ‰ ë„êµ¬(ë‰´ìŠ¤, ìœ íŠœë¸Œ, ì¸ë¬¼ì •ë³´, ë‚ ì”¨ ë“±)ë¥¼ ì‚¬ìš©í•´ì•¼ í•˜ë‚˜ìš”?

        â€» ì§ˆë¬¸ì— ì—°ì˜ˆì¸, ìœ ëª…ì¸, ìœ íŠœë²„, ë°©ì†¡ì¸ ë“± ì¸ë¬¼ì— ëŒ€í•œ ì •ë³´ê°€ ë“¤ì–´ìˆìœ¼ë©´ YESë¡œ íŒë‹¨í•´ì£¼ì„¸ìš”.

        ê²€ìƒ‰ ë„êµ¬ê°€ í•„ìš”í•œ ì§ˆë¬¸ì´ë©´ YES,  
        ê·¸ëƒ¥ ì¼ìƒì ì¸ ëŒ€í™”ë©´ NOë§Œ ì¶œë ¥í•´ì£¼ì„¸ìš”.

        ì§ˆë¬¸: "{question}"
        ë‹µë³€:
        """

    result = get_hard_llm(streaming=False).invoke(check_prompt)
    response = result.content.strip().upper()

    print(f"[ğŸ” needs_agent íŒë‹¨] â†’ {response}")

    return response == "YES"


# âœ… Chat ì²´ì¸ ìƒì„± í•¨ìˆ˜ (í›„ì²˜ë¦¬ ë²„ì „)
def get_chat_chain(callback):
    async def _chat(x):
        user_id = get_user_id(x)
        memory = get_user_memory(user_id)

        llm = get_soft_llm(streaming=True, callback=callback)
        history = memory.load_memory_variables({})["chat_history"]

        result = await llm.ainvoke(
            [
                SystemMessage(content=DUKSUNI_SYSTEM_PROMPT.strip()),
                *history,
                HumanMessage(content=x["input"]),
            ]
        )

        print("\nì‘ë‹µ ë©”ì‹œì§€ : ")
        print(result.content)

        memory.save_context({"input": x["input"]}, {"output": result.content})
        return {"output": result.content}

    return RunnableLambda(_chat)


# âœ… Agent ì²´ì¸ ìƒì„± í•¨ìˆ˜ (í›„ì²˜ë¦¬ ë²„ì „)
def get_agent_chain(callback):
    async def _agent(x):
        user_id = get_user_id(x)
        memory = get_user_memory(user_id)

        if "ì†Œì†ì‚¬" in x["input"]:
            answer = ""
            if "ì˜ì›…" in x["input"]:
                answer = "ì„ì˜ì›…ì˜ ì†Œì†ì‚¬ëŠ” ë¬¼ê³ ê¸°ë®¤ì§ì´ì•¼!"
            elif "ì°¬ì›" in x["input"]:
                answer = "ì´ì°¬ì›ì˜ ì†Œì†ì‚¬ëŠ” í‹°ì—”ì—”í„°í…Œì¸ë¨¼íŠ¸ì•¼!"
            
            if answer:  # ğŸ‘‰ ì´ë¦„ì´ ì¸ì‹ëœ ê²½ìš°ì—ë§Œ ì²˜ë¦¬
                friendly = await to_friendly_tone(answer)
                return {"output": friendly}

        if "ìƒì¼" in x["input"]:
            answer = ""
            if "ì˜ì›…" in x["input"]:
                answer = "ì„ì˜ì›… ìƒì¼ì€ 1991ë…„ 6ì›” 16ì¼ì´ì•¼!"
            elif "ì°¬ì›" in x["input"]:
                answer = "ì´ì°¬ì› ìƒì¼ì€ 1996ë…„ 11ì›” 1ì¼ì´ì•¼!"

            if answer:  # ğŸ‘‰ ì´ë¦„ì´ ì¸ì‹ëœ ê²½ìš°ì—ë§Œ ì²˜ë¦¬
                friendly = await to_friendly_tone(answer)
                return {"output": friendly}
            

        llm = get_llm(streaming=True, callback=callback)

        # prompt = react_prompt_kr.partial(
        #     tools="\n".join([f"{t.name}: {t.description}" for t in tools]),
        #     tool_names=", ".join([t.name for t in tools])
        # )
        # prompt = ReActPrompt().get_prompt(tools)

        prompt2 = hub.pull("hwchase17/react-chat")

        agent = create_react_agent(llm=llm, tools=tools, prompt=prompt2)

        executor = AgentExecutor(
            agent=agent,
            tools=tools,
            memory=memory,
            handle_parsing_errors=True,
            max_iterations=10,
            max_execution_time=30,
            return_exceptions=False,
            verbose=False,
            output_key="output",
        )
        del x["user_id"]
        result = await executor.ainvoke(x)
        # intermediate_steps = result.get("intermediate_steps", [])
        # seen = set()
        # unique_tool_outputs = []
        # for step in intermediate_steps:
        #     action = step[0]  # AgentAction
        #     output = step[1]  # Tool output (observation)
        #     # Exceptionì€ ë¬´ì‹œ
        #     if action.tool == "_Exception":
        #         continue

        #     key = (action.tool, str(action.tool_input))
        #     if key not in seen:
        #         seen.add(key)
        #         unique_tool_outputs.append(output)

        # search_summary = "\n".join(unique_tool_outputs)

        final_answer = extract_final_answer(result["output"])
        if final_answer == "Agent stopped due to iteration limit or time limit.":
            final_answer = "ë¯¸ì•ˆ ë‹¤ì‹œ ë¬¼ì–´ë´ì¤˜"

        # ì „ì²´ ë§¥ë½ ì—°ê²°
        # full_context = final_answer + "\n\n" + search_summary

        # print("\nğŸª„ [Final Answer ì¶”ì¶œ ê²°ê³¼]")
        # print(final_answer)

        friendly = await to_friendly_tone(final_answer)
        print("\nì‘ë‹µ ë©”ì‹œì§€")
        print(friendly)

        return {"output": friendly}

    return RunnableLambda(_agent)
