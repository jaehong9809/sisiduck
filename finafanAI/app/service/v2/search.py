# service/search.py
from googleapiclient.discovery import build
import feedparser
import os
from urllib.parse import quote
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from duckduckgo_search import DDGS
import warnings
from langchain_core._api.deprecation import LangChainDeprecationWarning
import requests
import re
from bs4 import BeautifulSoup
import requests

warnings.filterwarnings("ignore", category=LangChainDeprecationWarning)


def extract_topic(text: str) -> str:
    # ê°€ì¥ ë‹¨ìˆœí•œ ì˜ˆ: "XXX ìµœê·¼ ì†Œì‹", "XXX ë‰´ìŠ¤", "XXX ê·¼í™©" ë“±ì˜ íŒ¨í„´ ì œê±°
    return re.sub(
        r"(ì˜\s*)?(ìµœê·¼\s*ì†Œì‹|ë‰´ìŠ¤|ê·¼í™©|í™œë™)\s*(ì•Œë ¤ì¤˜|ì „í•´ì¤˜|ì—†ì–´\?|ì¢€ ì•Œë ¤ì¤„ë˜)?",
        "",
        text,
    ).strip()


def read_webpage(url: str) -> str:
    try:
        response = requests.get(url, timeout=5)
        soup = BeautifulSoup(response.text, "html.parser")

        # ëª¨ë“  <p> íƒœê·¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ, ê³µë°± ì œê±° ë° í•œ ì¤„ë¡œ ì •ë¦¬
        paragraphs = soup.find_all("p")
        cleaned_paragraphs = []

        for p in paragraphs:
            text = p.get_text().strip()
            if text:
                # ì¤„ë°”ê¿ˆ/ì¤‘ë³µ ê³µë°± ì œê±° â†’ í•œ ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ
                text = re.sub(r"\s+", " ", text)
                cleaned_paragraphs.append(text)

        # ë¬¸ë‹¨ ì‚¬ì´ëŠ” ë§ˆì¹¨í‘œ + ë„ì–´ì“°ê¸° í˜•íƒœë¡œ ì—°ê²°
        content = " ".join(cleaned_paragraphs)

        return content[:1000] if content else "ë³¸ë¬¸ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ìš”."

    except Exception as e:
        return f"ì›¹í˜ì´ì§€ ë‚´ìš©ì„ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆì–´ìš”: {str(e)}"


# ì›¹ ê²€ìƒ‰
def duckduckgo_search(query: str, max_results: int = 5) -> str:
    with DDGS() as ddgs:
        results = ddgs.text(query, max_results=max_results)
        return "\n".join(
            [f"{r['title']} - {r['href']}" for r in results if "href" in r]
        )

def shorten_url(long_url: str) -> str:
    api_url = f"http://tinyurl.com/api-create.php?url={long_url}"
    try:
        response = requests.get(api_url, timeout=3)
        if response.status_code == 200:
            return response.text
        else:
            return long_url  # ì‹¤íŒ¨í•˜ë©´ ì›ë˜ URL ë°˜í™˜
    except Exception:
        return long_url

# ë‰´ìŠ¤ ê²€ìƒ‰
def fast_news_search(query: str, max_results: int = 3) -> str:
    encoded_query = quote(query)  # ê³µë°± ë° í•œê¸€ ì¸ì½”ë”©
    url = f"https://news.google.com/rss/search?q={encoded_query}&hl=ko&gl=KR&ceid=KR:ko"

    feed = feedparser.parse(url)
    entries = feed.entries[:max_results]  # ğŸ‘‰ ìµœëŒ€ ê²°ê³¼ ìˆ˜ ë°˜ì˜

    if not entries:
        return "ê´€ë ¨ëœ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ì°¾ì§€ ëª»í–ˆì–´ã… ã… "

    results = [f"{entry.title} - ({shorten_url(entry.link)})" for entry in entries]

    return "\n".join(results)


# YouTube ê²€ìƒ‰
def youtube_search(query: str, max_results: int = 3) -> str:
    api_key = os.getenv("YOUTUBE_API_KEY")

    youtube = build("youtube", "v3", developerKey=api_key)

    request = youtube.search().list(
        q=query, part="snippet", type="video", maxResults=max_results
    )

    response = request.execute()

    results = []

    for item in response.get("items", []):
        video_id = item["id"]["videoId"]
        title = item["snippet"]["title"]
        url = f"https://www.youtube.com/watch?v={video_id}"
        results.append(f"{title} - {url}")

    return "\n".join(results)


# ì¸ë¬¼ ì •ë³´ ê²€ìƒ‰
embeddings = OpenAIEmbeddings()
person_db = Chroma(persist_directory="./person_db", embedding_function=embeddings)
person_retriever = person_db.as_retriever(search_kwargs={"k": 2})

def search_person_info(query: str) -> str:
    docs = person_retriever.get_relevant_documents(query)

    if not docs:
        return "í•´ë‹¹ ì—°ì˜ˆì¸ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì›¹ê²€ìƒ‰ ë„êµ¬ë¥¼ ì‚¬ìš©í•´ ë³´ì„¸ìš”."

    # ìƒìœ„ ìµœëŒ€ 3ê°œê¹Œì§€, ë„ˆë¬´ ë§ìœ¼ë©´ ìë¥´ê¸°
    results = []
    for doc in docs:
        content = doc.page_content.strip()
        source = doc.metadata.get("source", None)  # ì—†ì„ ìˆ˜ë„ ìˆìŒ
        if len(content) > 500:
            content = content[:500] + "..."
        if source:
            results.append(f"[ì¶œì²˜: {source}]\n{content}")
        else:
            results.append(content)

    return "ì—°ì˜ˆì¸ ì •ë³´ ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤:\n\n" + "\n\n".join(results)


def get_weather(city="Seoul"):
    url = f"https://api.openweathermap.org/data/2.5/weather?q=Seoul&appid=c03977953046419f6c73010097b1cbfa&units=metric&lang=kr"
    response = requests.get(url)
    data = response.json()

    if response.status_code != 200:
        return "ë‚ ì”¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

    weather = data["weather"][0]["description"]  # ë‚ ì”¨ ì„¤ëª…
    temp = data["main"]["temp"]  # ì˜¨ë„
    humidity = data["main"]["humidity"]  # ìŠµë„
    wind_speed = data["wind"]["speed"]  # í’ì† (m/s)
    cloud_percent = data["clouds"]["all"]  # êµ¬ë¦„ëŸ‰ (%)

    return (
        f"ëŒ€í•œë¯¼êµ­ ì„œìš¸ í˜„ì¬ ë‚ ì”¨ëŠ” '{weather}'ì´ê³ , "
        f"ì˜¨ë„ëŠ” {temp}ë„, ìŠµë„ëŠ” {humidity}%, "
        f"í’ì†ì€ ì´ˆì† {wind_speed}m, êµ¬ë¦„ëŸ‰ì€ {cloud_percent}%ì…ë‹ˆë‹¤."
    )
