from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.agents import Tool, AgentExecutor, create_react_agent
from langchain_core.runnables import RunnableLambda, RunnableBranch
from langchain_core.messages import HumanMessage, SystemMessage

from app.service.search2 import fast_news_search, youtube_search, search_person_info, duckduckgo_search
from app.service.prompts2 import get_react_prompt, DUKSUNI_SYSTEM_PROMPT

# âœ… LLM & Memory
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7, streaming=True)
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

# âœ… ë„êµ¬ ì •ì˜
tools = [
    Tool(name="ë‰´ìŠ¤ê²€ìƒ‰", func=fast_news_search, description="ìµœì‹  ë‰´ìŠ¤, ì—°ì˜ˆì¸ ê¸°ì‚¬ ë“± ì™¸ë¶€ ì •ë³´ë¥¼ ì°¾ì„ ë•Œ ì‚¬ìš©"),
    Tool(name="ë™ì˜ìƒê²€ìƒ‰", func=youtube_search, description="YouTubeì—ì„œ ì½˜ì„œíŠ¸ ì˜ìƒì´ë‚˜ ë°©ì†¡ í´ë¦½ì„ ë³´ê³  ì‹¶ì„ ë•Œ ì‚¬ìš©"),
    Tool(name="ì¸ë¬¼ì •ë³´ê²€ìƒ‰", func=search_person_info, description="ì—°ì˜ˆì¸ì˜ ìƒì¼, MBTI, ê³ í–¥ ë“± ê¸°ë³¸ ì •ë³´ê°€ í•„ìš”í•  ë•Œ ì‚¬ìš©"),
    Tool(name="ì›¹ê²€ìƒ‰", func=duckduckgo_search, description="ì¼ë°˜ ì›¹ ì •ë³´, ë¸”ë¡œê·¸, ì»¤ë®¤ë‹ˆí‹° ë“± ì „ì²´ ì›¹ ê²€ìƒ‰ì´ í•„ìš”í•  ë•Œ ì‚¬ìš©")
]

# âœ… ReAct Prompt ì„¸íŒ…
prompt = get_react_prompt().partial(
    system_prompt=DUKSUNI_SYSTEM_PROMPT,
    tools="\n".join([f"{t.name}: {t.description}" for t in tools]),
    tool_names=", ".join([t.name for t in tools])
)

# âœ… Agent ìƒì„±
agent = create_react_agent(llm=llm, tools=tools, prompt=prompt)
agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    memory=memory,
    verbose=True,
    handle_parsing_errors=False
)

# âœ… ë•ìˆœì´ ë§íˆ¬ í›„ì²˜ë¦¬
def to_friendly_tone(answer: str) -> str:
    prompt = f'ë‹¤ìŒ ë‚´ìš©ì„ ì¹œêµ¬ì²˜ëŸ¼ ë°˜ë§ë¡œ ë°”ê¿”ì¤˜. ì‚´ì§ ì£¼ì ‘ ì„ì–´ë„ ì¢‹ì•„~\n\n"{answer}"\n\në³€í™˜ëœ ë§íˆ¬:'
    result = llm.invoke([
        SystemMessage(content=DUKSUNI_SYSTEM_PROMPT.strip()),
        HumanMessage(content=prompt)
    ])
    return result.content

# âœ… LLMì´ íŒë‹¨: Agentê°€ í•„ìš”í•œ ì§ˆë¬¸ì¸ì§€?
def needs_agent(input: dict) -> bool:
    question = input["input"].strip()
    check_prompt = f"""
    ë‹¤ìŒ ì§ˆë¬¸ì€ ê²€ìƒ‰ ë„êµ¬(ë‰´ìŠ¤, ìœ íŠœë¸Œ, ì¸ë¬¼ì •ë³´ ë“±)ë¥¼ ì‚¬ìš©í•´ì•¼ í•˜ë‚˜ìš”?

    ê²€ìƒ‰ ë„êµ¬ê°€ í•„ìš”í•œ ì§ˆë¬¸ì´ë©´ YES,  
    ê·¸ëƒ¥ ì¼ìƒì ì¸ ëŒ€í™”ë©´ NOë§Œ ì¶œë ¥í•´ì£¼ì„¸ìš”.

    ì§ˆë¬¸: "{question}"
    ë‹µë³€:
    """
    result = llm.invoke(check_prompt)
    response = result.content.strip().upper()
    print(f"[ğŸ” needs_agent íŒë‹¨] â†’ {response}")
    return response == "YES"

# âœ… ë¶„ê¸°ìš© ì²´ì¸
needs_agent_chain = RunnableLambda(needs_agent)

# âœ… Agent ì²´ì¸ (ê²€ìƒ‰ ë„êµ¬ + ë•ìˆœì´ ë§íˆ¬)
agent_chain = RunnableLambda(lambda x: agent_executor.invoke(x)) | \
              RunnableLambda(lambda x: {"output": to_friendly_tone(x["output"])})

# âœ… Chat ì²´ì¸ (ì¼ìƒ ëŒ€í™” â†’ LLM ë°”ë¡œ í˜¸ì¶œ + ë•ìˆœì´ ë§íˆ¬)
chat_chain = RunnableLambda(lambda x: {
    "output": to_friendly_tone(
        llm.invoke([
            SystemMessage(content=DUKSUNI_SYSTEM_PROMPT.strip()),
            HumanMessage(content=x["input"])
        ]).content
    )
})

# âœ… ìµœì¢… í•˜ì´ë¸Œë¦¬ë“œ ì²´ì¸ (Agent vs Chat)
hybrid_chain = RunnableBranch(
    (needs_agent_chain, agent_chain),
    chat_chain  # Falseì¼ ë•ŒëŠ” ê·¸ëƒ¥ ë•ìˆœì´ë‘ ëŒ€í™”~
)
